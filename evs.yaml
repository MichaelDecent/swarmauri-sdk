PROJECTS:
  - NAME: "EVsProject"
    ROOT: "pkgs"
    PACKAGES:

      # Core Interfaces
      - NAME: "core/swarmauri_core"
        TEMPLATE_SET_OVERRIDE: "swarmauri_core"
        MODULES:
          - NAME: "IEvaluate"
            EXTRAS:
              PURPOSE: "Defines the contract for evaluation functions in EV pools."
              DESCRIPTION: "Interface requiring implementation of methods to evaluate program fitness."
              RESOURCE_KIND: "evaluators"
              REQUIREMENTS:
                - "Must implement method: def evaluate(self, program: Program, **kwargs) -> Tuple[float, Dict[str,Any]]"
                - "Must be stateless (no shared mutable or global state) to allow parallel execution"
                - "Must support multiple feature dimensions and return a scalar score plus metadata"
                - "Must document all input and output types explicitly (never use Any)"
                - "Must raise EvaluationError on failure"
              DEPENDENCIES:
                - "pkgs/core/swarmauri_core/programs/IProgram.py"
                - "pkgs/core/swarmauri_core/evaluator_results/IEvalResult.py"

          - NAME: "IPoolEvaluator"
            EXTRAS:
              PURPOSE: "Defines the contract for EV pools, managing sets of EVs."
              DESCRIPTION: "Interface requiring methods for dynamic registration, execution, and aggregation of evaluators."
              RESOURCE_KIND: "evaluator_pools"
              REQUIREMENTS:
                - "Must implement add_evaluator(ev: IEvaluate) and remove_evaluator(name: str)"
                - "Must provide evaluate_all(programs: Sequence[Program]) -> Sequence[EvaluationResult]"
                - "Must support async evaluation via async def evaluate_async(...)"
                - "Must expose aggregate(scores: Sequence[float]) -> float and allow pluggable aggregation"
                - "Must include initialize() and shutdown() lifecycle hooks for resource management"
              DEPENDENCIES:
                - "pkgs/core/swarmauri_core/evaluators/IEvaluate.py"
                - "pkgs/core/swarmauri_core/programs/IProgram.py"
                - "pkgs/core/swarmauri_core/evaluator_results/IEvalResult.py"

          - NAME: "IProgram"
            EXTRAS:
              PURPOSE: "Defines the contract for programs under evolution."
              DESCRIPTION: "Interface requiring methods to represent, diff, apply, and serialize a program."
              RESOURCE_KIND: "programs"
              REQUIREMENTS:
                - "Must implement diff(other: IProgram) -> Any"
                - "Must implement apply_diff(diff: Any) -> IProgram"
                - "Must implement validate() -> bool"
                - "Never use Any in public method signatures; document types explicitly"

          - NAME: "IEvalResult"
            EXTRAS:
              PURPOSE: "Defines the contract for evaluation results."
              DESCRIPTION: "Interface requiring accessors for score, metadata, and linkage to a program."
              RESOURCE_KIND: "evaluator_results"
              REQUIREMENTS:
                - "Must expose property score: float"
                - "Must expose property metadata: Dict[str,Any]"
                - "Must expose property program: IProgram"
                - "Must implement to_dict() -> Dict[str,Any]"

      # Base Classes
      - NAME: "base/swarmauri_base"
        TEMPLATE_SET_OVERRIDE: "swarmauri_base"
        MODULES:
          - NAME: "EvaluatorBase"
            EXTRAS:
              PURPOSE: "Provides reusable logic for evaluation function implementations."
              DESCRIPTION: "Abstract base implementing template methods, logging, and partial evaluation workflow."
              RESOURCE_KIND: "evaluators"
              INTERFACE_NAME: "IEvaluate"
              INTERFACE_FILE: "pkgs/core/swarmauri_core/evaluators/IEvaluate.py"
              REQUIREMENTS:
                - "Must inherit from IEvaluate"
                - "Must implement protected _compute_score(self, program: Program, **kwargs) -> Tuple[float, Dict]"
                - "Must wrap _compute_score in evaluate(), capturing execution time and exceptions"
                - "Must provide default metadata‐aware aggregation (e.g. average of sub-scores)"

          - NAME: "PoolEvaluatorBase"
            EXTRAS:
              PURPOSE: "Provides reusable logic for managing sets of evaluation functions."
              DESCRIPTION: "Abstract base implementing thread-safe registry, dispatch, and result aggregation."
              RESOURCE_KIND: "evaluator_pools"
              INTERFACE_NAME: "IPoolEvaluator"
              INTERFACE_FILE: "pkgs/core/swarmauri_core/evaluator_pools/IPoolEvaluator.py"
              REQUIREMENTS:
                - "Must inherit from IPoolEvaluator"
                - "Must implement _dispatch(self, programs: Sequence[Program]) -> Sequence[EvaluationResult]"
                - "Must provide default evaluate_all, evaluate_async, and aggregate implementations"
                - "Must manage a concurrent.futures.Executor for parallel runs"
                - "Must include pre_process and post_process hooks for inputs and outputs"

          - NAME: "ProgramBase"
            EXTRAS:
              PURPOSE: "Provides reusable logic for program representation and diff/apply."
              DESCRIPTION: "Abstract base implementing serialization, diffing, and validation workflows."
              RESOURCE_KIND: "programs"
              INTERFACE_NAME: "IProgram"
              INTERFACE_FILE: "pkgs/core/swarmauri_core/programs/IProgram.py"
              REQUIREMENTS:
                - "Must inherit from IProgram"
                - "Must implement serialize(), diff(), apply_diff(), validate()"
                - "Must include default JSON-based serialization"
                - "Must log diff operations for auditability"

          - NAME: "EvalResultBase"
            EXTRAS:
              PURPOSE: "Provides reusable logic for packaging evaluation results."
              DESCRIPTION: "Abstract base implementing score storage, metadata handling, and serialization."
              RESOURCE_KIND: "evaluator_results"
              INTERFACE_NAME: "IEvalResult"
              INTERFACE_FILE: "pkgs/core/swarmauri_core/evaluator_results/IEvalResult.py"
              REQUIREMENTS:
                - "Must inherit from IEvalResult"
                - "Must store score and metadata in protected attributes"
                - "Must implement to_dict() returning {'score': ..., 'metadata': ..., 'program': ...}"
                - "Must validate metadata keys against a schema"

      # Concrete Evaluation Functions and Models
      - NAME: "swarmauri_standard/swarmauri_standard"
        TEMPLATE_SET_OVERRIDE: "swarmauri_standard"
        MODULES:
          - NAME: "Program"
            EXTRAS:
              PURPOSE: "Concrete implementation of ProgramBase."
              DESCRIPTION: "Immutable program object supporting serialization, diff, and apply."
              BASE_NAME: "ProgramBase"
              RESOURCE_KIND: "programs"
              BASE_FILE: "pkgs/base/swarmauri_base/programs/ProgramBase.py"
              REQUIREMENTS:
                - "Must implement diff(), apply_diff(), validate() using ProgramBase defaults"
                - "Must include program ID and version in metadata"

          - NAME: "EvalResult"
            EXTRAS:
              PURPOSE: "Concrete implementation of EvalResultBase."
              DESCRIPTION: "Holds a program reference, scalar score, and metadata."
              BASE_NAME: "EvalResultBase"
              RESOURCE_KIND: "evaluator_results"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluator_results/EvalResultBase.py"
              REQUIREMENTS:
                - "Must link to the evaluated Program instance"
                - "Must enforce score ∈ ℝ and metadata keys are strings"

      # Standalone Evaluators
      - NAME: "swarmauri_evaluator_subprocess"
        TEMPLATE_SET_OVERRIDE: "swarmauri_standard_standalone"
        EXTRAS:
          AUTHORS:
            - NAME: "Michael Nwogha"
              EMAIL: "michael@swamauri.com"
          PURPOSE: "Evaluate program execution in isolated subprocesses for security and stability"
          DESCRIPTION: "Sandboxed subprocess package that runs programs with controlled inputs and captures execution metrics"
        MODULES:
          - NAME: "SubprocessEvaluator"
            EXTRAS:
              PURPOSE: "Runs target programs in isolated subprocesses and measures exit code, stdout, stderr."
              DESCRIPTION: "Spawns a sandboxed subprocess to execute a program binary or script, captures outputs, and returns status metrics."
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must launch the program in a secure, sandboxed subprocess"
                - "Must capture exit code, stdout, stderr, and runtime"
                - "Must return a scalar success score plus detailed metadata"

      - NAME: "swarmauri_evaluator_anyusage"
        TEMPLATE_SET_OVERRIDE: "swarmauri_standard_standalone"
        EXTRAS:
          AUTHORS:
            - NAME: "Michael Nwogha"
              EMAIL: "michael@swamauri.com"
          PURPOSE: "Evaluate code quality by detecting untyped Any references in Python code"
          DESCRIPTION: "Static analysis package for identifying and penalizing usage of the Any type in Python"
        MODULES:
          - NAME: "AnyTypeUsageEvaluator"
            EXTRAS:
              PURPOSE: "Detects usage or import of the `Any` type in Python source files."
              DESCRIPTION: "Parses Python files to find `typing.Any` imports or `Any` annotations and flags occurrences."
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must scan provided Python files for `from typing import Any` or inline `Any` usage"
                - "Must count and report number of `Any` occurrences"
                - "Must return a penalty score proportional to `Any` usage and metadata listing file paths and line numbers"

      - NAME: "swarmauri_evaluator_abstractmethods"
        TEMPLATE_SET_OVERRIDE: "swarmauri_standard_standalone"
        EXTRAS:
          AUTHORS:
            - NAME: "Michael Nwogha"
              EMAIL: "michael@swamauri.com"
          PURPOSE: "Enforce proper abstract method declarations in Python interfaces and base classes"
          DESCRIPTION: "Static analysis package that checks for proper @abstractmethod usage in abstract classes"
        MODULES:
          - NAME: "AbstractMethodsEvaluator"
            EXTRAS:
              PURPOSE: "Verifies that all methods in Python classes are marked as abstract when defined in an interface or base class."
              DESCRIPTION: "Parses Python source to ensure methods decorated with `@abstractmethod` in ABCs or interfaces."
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must identify classes inheriting from `abc.ABC` or similar"
                - "Must verify that all declared methods are decorated with `@abstractmethod`"
                - "Must report missing decorators with file and line metadata"
                - "Must return a compliance score and detailed metadata"

      - NAME: "swarmauri_evaluator_externalimports"
        TEMPLATE_SET_OVERRIDE: "swarmauri_standard_standalone"
        EXTRAS:
          AUTHORS:
            - NAME: "Michael Nwogha"
              EMAIL: "michael@swamauri.com"
          PURPOSE: "Track and evaluate external dependencies in Python code"
          DESCRIPTION: "Static analysis package that identifies non-standard library imports and reports dependency metrics"
        MODULES:
          - NAME: "ExternalImportsEvaluator"
            EXTRAS:
              PURPOSE: "Detects non–built-in Python imports in source files."
              DESCRIPTION: "Analyzes import statements to find modules not in the standard library."
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must parse import statements from Python files"
                - "Must cross-reference imports against the standard library module list"
                - "Must list any third-party or external imports with file and line metadata"
                - "Must return a score penalizing external dependencies and detailed metadata"

      - NAME: "swarmauri_evaluatorpool_accessibility"
        TEMPLATE_SET_OVERRIDE: "swarmauri_standard_standalone"
        EXTRAS:
          AUTHORS:
            - NAME: "Michael Nwogha"
              EMAIL: "michael@swamauri.com"
          PURPOSE: "Measure text readability and accessibility using multiple standard metrics"
          DESCRIPTION: "Comprehensive text analysis package providing multiple readability evaluators and a unified pool"
        MODULES:
          - NAME: "AutomatedReadabilityIndexEvaluator"
            EXTRAS:
              PURPOSE: "Computes the Automated Readability Index (ARI) score of text files."
              DESCRIPTION: |
                Parses text content, counts characters, words, and sentences,
                and applies the ARI formula to estimate U.S. grade level readability.
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must load text from Program artifacts (e.g., .html, .md, .txt)"
                - "Must count characters, words, sentences accurately"
                - "Must compute ARI = 4.71*(characters/words) + 0.5*(words/sentences) - 21.43"
                - "Must return (score=float, metadata={'chars':…, 'words':…, 'sentences':…})"

          - NAME: "ColemanLiauIndexEvaluator"
            EXTRAS:
              PURPOSE: "Computes the Coleman–Liau Index for text readability."
              DESCRIPTION: |
                Extracts letters and sentences from text, normalizes per 100 words,
                then applies the Coleman–Liau formula for grade level.
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must count letters, words, sentences"
                - "Must compute L = 0.0588*L - 0.296*S - 15.8 where L = letters/100 words, S = sentences/100 words"
                - "Must return grade level score and raw counts in metadata"

          - NAME: "FleschKincaidGradeEvaluator"
            EXTRAS:
              PURPOSE: "Computes the Flesch–Kincaid Grade Level of text."
              DESCRIPTION: |
                Analyzes average sentence length and average syllables per word,
                then applies the FKGL formula to output a U.S. grade level.
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must count sentences, words, syllables"
                - "Must compute FKGL = 0.39*(words/sentences) + 11.8*(syllables/words) - 15.59"
                - "Must return score and breakdown in metadata"

          - NAME: "FleschReadingEaseEvaluator"
            EXTRAS:
              PURPOSE: "Computes the Flesch Reading Ease score."
              DESCRIPTION: |
                Uses average sentence length and syllables per word
                to generate a 0–100 readability score (higher is easier to read).
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must compute FRE = 206.835 - 1.015*(words/sentences) - 84.6*(syllables/words)"
                - "Must return (score, metadata including sentence and syllable stats)"

          - NAME: "GunningFogEvaluator"
            EXTRAS:
              PURPOSE: "Computes the Gunning Fog Index of text."
              DESCRIPTION: |
                Identifies complex words (3+ syllables), computes average sentence length
                and percent complex words, then applies the Gunning Fog formula.
              BASE_NAME: "EvaluatorBase"
              RESOURCE_KIND: "evaluators"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluators/EvaluatorBase.py"
              REQUIREMENTS:
                - "Must count sentences, words, complex words"
                - "Must compute GFI = 0.4*((words/sentences) + 100*(complex_words/words))"
                - "Must return index score and counts in metadata"

          - NAME: "AccessibilityEvaluatorPool"
            EXTRAS:
              PURPOSE: "Runs a suite of accessibility and readability EVs across a Program."
              DESCRIPTION: |
                Iterates through all files in a Program and invokes each of the
                Accessibility & Readability evaluators, then aggregates a combined score.
              BASE_NAME: "PoolEvaluatorBase"
              RESOURCE_KIND: "evaluator_pools"
              BASE_FILE: "pkgs/base/swarmauri_base/evaluator_pools/PoolEvaluatorBase.py"
              DEPENDENCIES:
                - "pkgs/standards/swarmauri_evaluatorpool_accessibility/AutomatedReadabilityIndexEvaluator.py"
                - "pkgs/standards/swarmauri_evaluatorpool_accessibility/ColemanLiauIndexEvaluator.py"
                - "pkgs/standards/swarmauri_evaluatorpool_accessibility/FleschKincaidGradeEvaluator.py"
                - "pkgs/standards/swarmauri_evaluatorpool_accessibility/FleschReadingEaseEvaluator.py"
                - "pkgs/standards/swarmauri_evaluatorpool_accessibility/GunningFogEvaluator.py"
              REQUIREMENTS:
                - "Must asynchronously invoke each evaluator per file"
                - "Must support custom weighting between accessibility violations and readability scores"
                - "Must aggregate into an overall compliance/readability metric"
                - "Must expose per‐evaluator and per‐file metadata in the final report"
                - "Must manage executor lifecycle with initialize() and shutdown()"